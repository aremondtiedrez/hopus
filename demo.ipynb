{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ad85b80b-8647-4d08-aff5-4d6a309c641d",
      "metadata": {
        "id": "ad85b80b-8647-4d08-aff5-4d6a309c641d"
      },
      "source": [
        "# HOPUS\n",
        "\n",
        "HOPUS (**HO**using **P**ricing **U**tilitie**S**) contains a variety of routines used to predict real estate prices.\n",
        "\n",
        "This notebook highlights what HOPUS can do, namely\n",
        "- clean the raw data,\n",
        "- train a variety of models for the prediction of real estate prices,\n",
        "- evaluate the performance of these models, and\n",
        "- make predictions on new data and display the predictions on a map."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Technical preliminaries"
      ],
      "metadata": {
        "id": "l1iHNrt5zJXt"
      },
      "id": "l1iHNrt5zJXt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install the `hopus` package and import it.\n",
        "We also import other libraries, or tools from other libraries, which will be of use."
      ],
      "metadata": {
        "id": "FFmyUbVhx7v9"
      },
      "id": "FFmyUbVhx7v9"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/aremondtiedrez/hopus.git"
      ],
      "metadata": {
        "id": "B5GQ_gc8yBAL"
      },
      "id": "B5GQ_gc8yBAL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import secrets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import hopus"
      ],
      "metadata": {
        "id": "sDNVK6nSyBa4"
      },
      "id": "sDNVK6nSyBa4",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data pipeline"
      ],
      "metadata": {
        "id": "ThQEWOtQzO_U"
      },
      "id": "ThQEWOtQzO_U"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data used here is obtained via\n",
        "the [RentCast API](https://www.rentcast.io/api).\n",
        "\n",
        "Our data pipeline proceeds in three steps.\n",
        "\n",
        "1. We clean the raw data. The data provided by the RentCast API is already\n",
        "   of good quality, but nonetheless some gaps in the data are present which\n",
        "   we must account for. Moreover, the RentCast data comes in the form of a JSON\n",
        "   file which, due to its hierarchical and nested nature, needs to be untangled\n",
        "   in order to put the data into a more digestible tabular format.\n",
        "\n",
        "   Another important step at this stage has to do with the evolution of prices\n",
        "   over time. Predicting how real estates prices evolve over time is\n",
        "   an important, complex, and challenging question. We therefore avoid it\n",
        "   entirely here. Instead we use a national (U.S.) home price index\n",
        "   (known as HPI below) to \"normalize\" home prices over time and\n",
        "   then predict these \"normalized\" home prices.\n",
        "\n",
        "2. We remove outliers and samples which are missing key features.\n",
        "\n",
        "   Why remove outliers? Because, when an unusual real estate sale listing\n",
        "   is encountered, it is best to gather more information and proceed with caution. For example: some houses are bought at prices significantly above\n",
        "   market rates by developers than then raze them and then redevelop the lot\n",
        "   into a multi-apartment building. Without access to additional information,\n",
        "   such as zoning information which indicates neighbourhoods where such\n",
        "   redevelomenpts are possible, or such as demographic and economic data, which\n",
        "   indicates where such redevelopments are likely to be profitable,\n",
        "   it is unlikely that our model could reliably predict the sale price\n",
        "   of such properties.\n",
        "\n",
        "   Why remove samples which are missing key features? For two reasons.\n",
        "   First, because by *key* features we mean things such as\n",
        "   the number of bathrooms or the lot size. These are quantities that\n",
        "   can always be gathered \"by-hand\".\n",
        "   Second, because the overwhelming majority of real estate listings data\n",
        "   accessed via the RentCast API contain all of the features we label\n",
        "   as key features. Only about 5% of the listings fails to do so.\n",
        "\n",
        "3. We split the data into a training set and a test set.\n",
        "   (Nothing about this step is specific to the data or problem at hand.)"
      ],
      "metadata": {
        "id": "Rd5kKBPevFkF"
      },
      "id": "Rd5kKBPevFkF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "hpi = hopus.preprocessing.home_price_index.load_demo_data()\n",
        "listings_data = hopus.preprocessing.property_listings.load_demo_data()\n",
        "\n",
        "# Pre-process the data, i.e. clean the data\n",
        "hopus.preprocessing.home_price_index.preprocess(hpi)\n",
        "listings_data = hopus.preprocessing.property_listings.preprocess(listings_data, hpi)\n",
        "\n",
        "# Remove outliers and listings which are missing key features\n",
        "hopus.preprocessing.property_listings.drop_outliers(listings_data)\n",
        "hopus.preprocessing.property_listings.drop_missing_key_features(listings_data)\n",
        "\n",
        "# Split the data into training and testing sets.\n",
        "# The training and test data obtained by using the seed below\n",
        "# are already saved in the HOPUS package, to enable easy access\n",
        "# later in this notebook.\n",
        "# (The seed used was randomly generated via secrets.randbits)\n",
        "seed = 726109520\n",
        "training_data, test_data = train_test_split(listings_data, train_size=0.8, shuffle=True, random_state=seed)"
      ],
      "metadata": {
        "id": "EcluFZdLu5_2"
      },
      "id": "EcluFZdLu5_2",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training models"
      ],
      "metadata": {
        "id": "9UFV9cHk2ReZ"
      },
      "id": "9UFV9cHk2ReZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pedagogical prelude: predicting prices (and not *log*-prices)"
      ],
      "metadata": {
        "id": "lAR1fUif2VXk"
      },
      "id": "lAR1fUif2VXk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ultimately we will seek to train models that predict *log*-prices.\n",
        "After all, a prediction error of \\$50,000 is never desirable,\n",
        "but it is much easier to swallow on a property whose value is around \\$1,000,000\n",
        "than on a property whose value is about \\$200,000.\n",
        "\n",
        "In other words: what we care about here is not the *absolute* error\n",
        "but the error *relative* to the property's value. In some sense,\n",
        "that is precisely what predicting *log*-prices will take care of:\n",
        "an absolute error in the prediction of the *log*-price is really\n",
        "a relative error in the prediction of the price.\n",
        "\n",
        "That being said, for pedagogical and expository purposes it is a good idea\n",
        "to start by predicting prices, instead of log-prices, since the magnitude\n",
        "of the errors obtained are much more readily interpretable. In other words:\n",
        "it is immediately clear what a prediction error of \\$50,000 means,\n",
        "when predicting the price. It is less clear what a prediction error of 0.13\n",
        "means when predicting the log-price."
      ],
      "metadata": {
        "id": "JN5sKjVE2b00"
      },
      "id": "JN5sKjVE2b00"
    },
    {
      "cell_type": "code",
      "source": [
        "# First we load the training data\n",
        "training_data = hopus.demo.load_training_data()"
      ],
      "metadata": {
        "id": "4FWX49AuX5G2"
      },
      "id": "4FWX49AuX5G2",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The `Baseline` model"
      ],
      "metadata": {
        "id": "3IVjObFfX5Vt"
      },
      "id": "3IVjObFfX5Vt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we do anything fancier, we should establish a baseline to see\n",
        "how well \"reasonable guesses\" perform when predicting home prices.\n",
        "And what is a common way to guess the price of a house? Well, you would look\n",
        "at nearby houses, see what their price-per-square-foot is, and use that\n",
        "to predict the price of the house of interest. That is exactly how\n",
        "the `Baseline` model proceeds, where \"nearby houses\" means \"all houses with\n",
        "the same ZIP code\"."
      ],
      "metadata": {
        "id": "w_310y_KX9S9"
      },
      "id": "w_310y_KX9S9"
    },
    {
      "cell_type": "code",
      "source": [
        "from hopus.models import Baseline"
      ],
      "metadata": {
        "id": "zLVU2qTVYDLI"
      },
      "id": "zLVU2qTVYDLI",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model = Baseline()\n",
        "model.fit(features=training_data, target=None)\n",
        "\n",
        "# Compute and report the training error\n",
        "train_mse = model.evaluate(features=training_data, target=training_data[\"price\"])\n",
        "train_rmse = np.sqrt(train_mse)\n",
        "print(f\"Training error: ${train_rmse / 1_000:.0f}k\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3gOFlpuYGna",
        "outputId": "68c30f1d-8f0c-4931-b80c-050785fb49e6"
      },
      "id": "d3gOFlpuYGna",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training error: $154k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Of course, the training error underestimates the true error.\n",
        "# Instead we can use the cross-validation error to better estimate the true error.\n",
        "seed = secrets.randbits(32)\n",
        "train_cv_mse, dev_cv_mse, trained_models = hopus.evaluation.cv_evaluation(Baseline, training_data, training_data[\"price\"], n_splits=10, seed=seed)\n",
        "train_cv_rmse, dev_cv_rmse = np.sqrt(np.array([train_cv_mse, dev_cv_mse]))\n",
        "print(f\"Cross-validation error: ${dev_cv_rmse / 1_000:.0f}k\")"
      ],
      "metadata": {
        "id": "0LPqM5563rKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec5d00d-e8f9-480a-cea7-dbf349467483"
      },
      "id": "0LPqM5563rKa",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation error: $155k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The `LinearRegression` model"
      ],
      "metadata": {
        "id": "lFFDso07YiB0"
      },
      "id": "lFFDso07YiB0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Baseline` model above cares only about geography, and about\n",
        "where a home is located. (That is, after all, the prime directive\n",
        "of real estate: \"Location, location, location\".)\n",
        "\n",
        "We could also estimate the price of a house in a completely different way,\n",
        "dismissing location entirely. We would instead look at quantities such as\n",
        "the number of bedrooms, the number of bathrooms, the number of floors, or\n",
        "the year the house was built, and predict the price based on a combination\n",
        "of these values (newer houses are more expensive, those with fewer bedrooms\n",
        "are cheaper).\n",
        "\n",
        "This is what the `LinearRegression` model does. It also also makes its best\n",
        "effort to predict the price if that price varies only *linearly* with each\n",
        "of the quantities mentionned above. (This is, of course, unreasonable: the seventeenth bedroom is not as valuable as the second one. Nonetheless, this\n",
        "model remains valuable.)\n",
        "\n",
        "Technical note: the `hopus.models.LinearRegression` class is a thin wrapper\n",
        "around the . The wrapper is helpful in that it allows using the same interface\n",
        "to access both the `Baseline` model above and\n",
        "this `LinearRegression` model here."
      ],
      "metadata": {
        "id": "OkDdfUeEYkwS"
      },
      "id": "OkDdfUeEYkwS"
    },
    {
      "cell_type": "code",
      "source": [
        "from hopus.models import LinearRegression"
      ],
      "metadata": {
        "id": "nWND3OGYb49E"
      },
      "id": "nWND3OGYb49E",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An extra dose of data pre-processing is done here since\n",
        "# the linear regression model will only depend on features\n",
        "# deemed to be \"key\" features.\n",
        "\n",
        "# WARNING: Running this cell more than once may cause issues.\n",
        "# If so, simply reload the training data above before re-running this cell.\n",
        "\n",
        "hopus.preprocessing.property_listings.group_columns(training_data)"
      ],
      "metadata": {
        "id": "KjMLzFIocI4S"
      },
      "id": "KjMLzFIocI4S",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the features and target\n",
        "features = training_data[\"keyPredictionFeatures\"]\n",
        "target = training_data[(\"target\", \"price\")]\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(features, target)\n",
        "\n",
        "# Compute and report the training error\n",
        "train_mse = model.evaluate(features, target)\n",
        "train_rmse = np.sqrt(train_mse)\n",
        "print(f\"Training error: ${train_rmse / 1_000:.0f}k\")"
      ],
      "metadata": {
        "id": "G0EKoyr1YkWx",
        "outputId": "54d37c1b-7293-46d3-8b03-e8bd33a32edd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "G0EKoyr1YkWx",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training error: $148k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Once again, the training error underestimates the true error.\n",
        "# We use the cross-validation error to better estimate the true error.\n",
        "seed = secrets.randbits(32)\n",
        "train_cv_mse, dev_cv_mse, trained_models = hopus.evaluation.cv_evaluation(LinearRegression, features, target, n_splits=10, seed=seed)\n",
        "train_cv_rmse, dev_cv_rmse = np.sqrt(np.array([train_cv_mse, dev_cv_mse]))\n",
        "print(f\"Cross-validation error: ${dev_cv_rmse / 1_000:.0f}k\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5b-vjCec2F8",
        "outputId": "39e65a60-c441-41fc-caa0-d06b86a4ead7"
      },
      "id": "e5b-vjCec2F8",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation error: $152k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The `BoostedTrees` model"
      ],
      "metadata": {
        "id": "WRMyMdbIc_98"
      },
      "id": "WRMyMdbIc_98"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two models above, `Baseline` and `LinearRegression`,\n",
        "perform similarly even though their approaches are very different:\n",
        "the former focuses solely on geographical location, the latter eschews it\n",
        "entirely!\n",
        "\n",
        "We now train a more complicated model. Since the data is *tabular*,\n",
        "a reasonable class of models to use are *decision trees*.\n",
        "In particular, the `BoostedTrees` model discussed here\n",
        "leverages `xgboost`, a popular library for *boosted* decision trees\n",
        "(in short, boosted decision trees repeatedly train one decision tree\n",
        "after another, focusing on the samples on which the previous trees\n",
        "are producing large errors).\n",
        "\n",
        "The good news is that, even though this `BoostedTrees` model\n",
        "is built on a completely different library than the previous two models,\n",
        "the `hopus.models` interface stays the same."
      ],
      "metadata": {
        "id": "CF43H6gYdVNj"
      },
      "id": "CF43H6gYdVNj"
    },
    {
      "cell_type": "code",
      "source": [
        "from hopus.models import BoostedTrees"
      ],
      "metadata": {
        "id": "_fyq-5pdets5"
      },
      "id": "_fyq-5pdets5",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the features and target\n",
        "features = training_data[[\"keyPredictionFeatures\", \"auxiliaryPredictionFeatures\"]]\n",
        "target = training_data[(\"target\", \"price\")]\n",
        "\n",
        "# One of the appeal of the xgboost library is\n",
        "# that additional performance can be squeezed out\n",
        "# of judicious hyper-parameter choices.\n",
        "# The hyper-parameter below were chosen after\n",
        "# a careful hyper-parameter search.\n",
        "hyperparameters = {\n",
        "    \"max_depth\": 5,\n",
        "    \"min_child_weight\": 3,\n",
        "    \"gamma\": 0.2,\n",
        "    \"subsample\": 0.6,\n",
        "    \"colsample_bytree\": 0.9,\n",
        "    \"reg_lambda\": 0.001,\n",
        "    \"scale_pos_weight\": 1,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"n_estimators\": 5_000,\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "model = BoostedTrees(**hyperparameters)\n",
        "model.fit(features, target)\n",
        "\n",
        "# Compute and report the training error\n",
        "train_mse = model.evaluate(features, target)\n",
        "train_rmse = np.sqrt(train_mse)\n",
        "print(f\"Training error: ${train_rmse / 1_000:.1f}k\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDVe2qE7eZpR",
        "outputId": "a2ebf12f-b791-4f96-edc8-83b2e0658936"
      },
      "id": "JDVe2qE7eZpR",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training error: $13.0k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# That training error looks amazing! But, be careful:\n",
        "# here the training error grossly underestimates the true error.\n",
        "# Once again, we use the cross-validation error to better estimate the true error.\n",
        "# (This may take between about a minute to run.)\n",
        "seed = secrets.randbits(32)\n",
        "train_cv_mse, dev_cv_mse, trained_models = hopus.evaluation.cv_evaluation(BoostedTrees, features, target, n_splits=10, seed=seed, hyperparameters=hyperparameters)\n",
        "train_cv_rmse, dev_cv_rmse = np.sqrt(np.array([train_cv_mse, dev_cv_mse]))\n",
        "print(f\"Cross-validation error: ${dev_cv_rmse / 1_000:.0f}k\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vof41TuLfN4p",
        "outputId": "3c0fe951-ff33-4aaa-efce-2a0d6f60e820"
      },
      "id": "Vof41TuLfN4p",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation error: $132k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting *log*-prices"
      ],
      "metadata": {
        "id": "ciDy2tVBiKjz"
      },
      "id": "ciDy2tVBiKjz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned earlier, what we are really after\n",
        "is the prediction of *log*-prices, and not merely prices.\n",
        "\n",
        "Nonetheless, we proceed as above otherwise and compare\n",
        "the performance of the `Baseline`, `LinearRegression`,\n",
        "and `BoostedTrees` models."
      ],
      "metadata": {
        "id": "BNw7EVFKiM7U"
      },
      "id": "BNw7EVFKiM7U"
    },
    {
      "cell_type": "code",
      "source": [
        "# We start with the Baseline model.\n",
        "# To avoid a mismatch between how the data is formatted and\n",
        "# what the Baseline model expects, we reload the training data\n",
        "training_data = hopus.demo.load_training_data()\n",
        "\n",
        "# Train a Baseline model and, if desired, save it\n",
        "# Note: the Baseline model is trained the same way whether we seek\n",
        "# to predict prices or log-prices. The difference comes later,\n",
        "# namely when this model is evaluated.\n",
        "model = Baseline()\n",
        "model.fit(training_data, None)\n",
        "#model.save(\"trained_baseline.csv\")\n",
        "\n",
        "# Compute the cross-validation error of the Baseline model\n",
        "seed = secrets.randbits(32)\n",
        "train_cv_mse, test_cv_mse, trained_models = hopus.evaluation.cv_evaluation(Baseline, training_data, training_data[\"logPrice\"], n_splits=10, seed=seed, target_type=\"log_price\")\n",
        "train_cv_rmse, test_cv_rmse = np.sqrt(np.array([train_cv_mse, test_cv_mse]))\n",
        "print(f\"Cross-validation test error: {test_cv_rmse:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69dfrwq3isTg",
        "outputId": "6cd331de-2e37-41f5-a434-ef871bb0a9c5"
      },
      "id": "69dfrwq3isTg",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation test error: 0.317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We now turn our attention to the LinearRegression and BoostedTrees models.\n",
        "# They expect the data in a different format than the Baseline model,\n",
        "# so we process the training data first.\n",
        "\n",
        "# WARNING: Running this cell more than once may cause issues.\n",
        "# If so, simply relo\n",
        "hopus.preprocessing.property_listings.group_columns(training_data)"
      ],
      "metadata": {
        "id": "vIvx4-xYkFCZ"
      },
      "id": "vIvx4-xYkFCZ",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, the LinearRegression model.\n",
        "\n",
        "# Extract the features and target\n",
        "features = training_data[\"keyPredictionFeatures\"]\n",
        "target = training_data[(\"target\", \"logPrice\")]\n",
        "\n",
        "# Train a LinearRegression model, and, if desired, save it\n",
        "model = LinearRegression()\n",
        "model.fit(features, target)\n",
        "#model.save(\"trained_linear_regression.npz\")\n",
        "\n",
        "# Compute the cross-validation error of the LinearRegression model\n",
        "seed = secrets.randbits(32)\n",
        "train_cv_mse, test_cv_mse, trained_models = hopus.evaluation.cv_evaluation(LinearRegression, features, target, n_splits=10, seed=seed)\n",
        "train_cv_rmse, test_cv_rmse = np.sqrt(np.array([train_cv_mse, test_cv_mse]))\n",
        "print(f\"Cross-validation test error: {test_cv_rmse:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSiHNyqYkZmh",
        "outputId": "e2665027-6420-4e44-c8a9-9723751ad5f8"
      },
      "id": "FSiHNyqYkZmh",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation test error: 0.281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second, the BoostedTrees model.\n",
        "\n",
        "# Extract the features and target\n",
        "features = training_data[[\"keyPredictionFeatures\", \"auxiliaryPredictionFeatures\"]]\n",
        "target = training_data[(\"target\", \"logPrice\")]\n",
        "\n",
        "# Once again, the hyper-parameter below were chosen\n",
        "# after a careful hyper-parameter search\n",
        "hyperparameters = {\n",
        "    \"max_depth\": 3,\n",
        "    \"min_child_weight\": 5,\n",
        "    \"gamma\": 0.1,\n",
        "    \"subsample\": 0.8,\n",
        "    \"colsample_bytree\": 0.4,\n",
        "    \"reg_lambda\": 1,\n",
        "    \"scale_pos_weight\": 1,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"n_estimators\": 5_000,\n",
        "}\n",
        "\n",
        "# Train a `BoostedTrees` model and, if desired, save ti\n",
        "model = BoostedTrees(**hyperparameters)\n",
        "model.fit(features, target)\n",
        "#model.save(\"trained_boosted_trees.json\")\n",
        "\n",
        "# Compute the cross-validation error of the BoostedTrees model\n",
        "seed = secrets.randbits(32)\n",
        "train_cv_mse, test_cv_mse, trained_models = hopus.evaluation.cv_evaluation(BoostedTrees, features, target, n_splits=10, seed=seed)\n",
        "train_cv_rmse, test_cv_rmse = np.sqrt(np.array([train_cv_mse, test_cv_mse]))\n",
        "print(f\"Cross-validation test error: {test_cv_rmse:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uYAFxuClC4J",
        "outputId": "e7cc028d-10d7-429d-a246-fcf1f4306bc8"
      },
      "id": "-uYAFxuClC4J",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation test error: 0.259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model selection**: The best performing model is the `BoostedTree` model,\n",
        "and so that is the model we will use going forward."
      ],
      "metadata": {
        "id": "vaEoAUBul8b_"
      },
      "id": "vaEoAUBul8b_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the chosen model"
      ],
      "metadata": {
        "id": "rO68YgvMmL0j"
      },
      "id": "rO68YgvMmL0j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the best performing model is the `BoostedTrees` model,\n",
        "that is the one we will use going forward.\n",
        "\n",
        "Keep in mind, however, that since we \"optimized\" over the cross-validation\n",
        "error, namely choosing the model with the lowest such error, we can no longer\n",
        "use that error as a good estimate of the true error.\n",
        "\n",
        "Instead, we bring back the *test* data to do so."
      ],
      "metadata": {
        "id": "x8uiY2R7mU43"
      },
      "id": "x8uiY2R7mU43"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data and process it\n",
        "test_data = hopus.demo.load_test_data()\n",
        "hopus.preprocessing.property_listings.group_columns(test_data)\n",
        "\n",
        "# Extract the features and target\n",
        "test_features = test_data[[\"keyPredictionFeatures\", \"auxiliaryPredictionFeatures\"]]\n",
        "test_target = test_data[(\"target\", \"logPrice\")]"
      ],
      "metadata": {
        "id": "In8hyeLGm2w2"
      },
      "id": "In8hyeLGm2w2",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained BoostedTrees model\n",
        "model = hopus.demo.load_trained_model(\"BoostedTrees\")\n",
        "\n",
        "# Typically, to load a model we would proceed as follows.\n",
        "# We would first create the (untrained) model, then\n",
        "# load its trained parameters from `filename`:\n",
        "# model = BoostedTrees()\n",
        "# model.load(filename)\n",
        "\n",
        "# Here, for demonstration purposes, a trained Baseline model\n",
        "# and a trained LinearRegression model can be loaded as follows:\n",
        "# model = hopus.demo.load_trained_model(\"Baseline\")\n",
        "# model = hopus.demo.load_trained_model(\"LinearRegression\")"
      ],
      "metadata": {
        "id": "-Nlq6-Hzm8QJ"
      },
      "id": "-Nlq6-Hzm8QJ",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate the error by computing the test error\n",
        "test_mse = model.evaluate(test_features, test_target)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "print(f\"Test error: {test_rmse:.3f}\")"
      ],
      "metadata": {
        "id": "mzrYmB0YnVqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08e9c62b-2352-4175-b0fd-743d7c25f8dd"
      },
      "id": "mzrYmB0YnVqj",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test error: 0.228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictions"
      ],
      "metadata": {
        "id": "pY-cgOUVwzgR"
      },
      "id": "pY-cgOUVwzgR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have selected a model and estimated its error,\n",
        "we can make predictions over new data and display the predictions\n",
        "on a geographical map."
      ],
      "metadata": {
        "id": "JvAZW2ZxxRo2"
      },
      "id": "JvAZW2ZxxRo2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load new data (here, for simplicity, we use the test data as new data)\n",
        "data = hopus.demo.load_test_data()\n",
        "geographical_data = data[[\"latitude\", \"longitude\"]]\n",
        "\n",
        "# Load the model\n",
        "model = hopus.demo.load_trained_model(\"BoostedTrees\")\n",
        "\n",
        "# Make predictions\n",
        "hopus.preprocessing.property_listings.group_columns(data)\n",
        "features = data[[\"keyPredictionFeatures\", \"auxiliaryPredictionFeatures\"]]\n",
        "predictions = np.exp(model.predict(features))\n",
        "\n",
        "# Format the predictions and combine them with the geographical data\n",
        "predictions = pd.Series(predictions)\n",
        "predictions.name = \"predictedPrice\"\n",
        "geographical_predictions = pd.concat([geographical_data, predictions], axis=1)\n",
        "\n",
        "# Save the geographical data as a CSV file\n",
        "geographical_data.to_csv(\"predictions.csv\", index=False)"
      ],
      "metadata": {
        "id": "tJ7_GquixAwI"
      },
      "id": "tJ7_GquixAwI",
      "execution_count": 35,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}